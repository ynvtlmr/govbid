### **1. Project Overview**

- **Goal:** A fully automated "Fetch Filter Draft" pipeline for government contracts.
- **Location:** `C:\GovBids`
- **Cost:** **$0.00** (using Google AI Studio Free Tier).
- **Tech Stack:**
- **Language:** Python 3.10+
- **AI Engine:** Google Generative AI SDK (`gemini-1.5-flash` & `gemini-1.5-pro`)
- **Database:** SQLite + SQLModel
- **UI:** Streamlit
- **Browser:** Playwright (for scraping sites without RSS)

---

### **2. Directory Structure**

Create this exact folder structure:

```text
C:\GovBids\
‚îÇ
‚îú‚îÄ‚îÄ data\                  # PDF storage
‚îú‚îÄ‚îÄ db\                    # Database storage
‚îú‚îÄ‚îÄ logs\                  # Text logs
‚îú‚îÄ‚îÄ services\              # Microservices
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ harvester.py       # Downloads PDFs from RSS/Web
‚îÇ   ‚îú‚îÄ‚îÄ processor.py       # The "Gemini Brain" (Filters & Drafts)
‚îú‚îÄ‚îÄ db_manager.py          # Database Schema
‚îú‚îÄ‚îÄ app.py                 # Dashboard
‚îú‚îÄ‚îÄ main.py                # Supervisor Script
‚îú‚îÄ‚îÄ .env                   # Stores GOOGLE_API_KEY
‚îî‚îÄ‚îÄ requirements.txt       # Dependencies

```

---

### **Phase 1: Environment & API Setup**

1. **Get API Key:**

- Go to [Google AI Studio](https://aistudio.google.com/).
- Click "Get API Key" -> "Create API Key in new project".
- Copy the key.

2. **Setup Python:**
   Open PowerShell in `C:\GovBids`:

```powershell
python -m venv venv
.\venv\Scripts\activate

```

3. **Install Dependencies:**
   Create `requirements.txt`:

```text
google-generativeai
sqlmodel
streamlit
feedparser
requests
playwright
python-dotenv
watchdog

```

Run:

```powershell
pip install -r requirements.txt
playwright install

```

4. **Create `.env`:**
   Create a file named `.env` in the root:

```text
GOOGLE_API_KEY="AIzaSy..."

```

---

### **Phase 2: The Nervous System (Database)**

**File:** `db_manager.py`
This handles data storage and prevents file corruption using WAL mode.

```python
from sqlmodel import SQLModel, Field, create_engine, Session
from datetime import datetime
import os

# Ensure DB directory exists
os.makedirs("db", exist_ok=True)

sqlite_url = "sqlite:///C:/GovBids/db/bids.db"
# check_same_thread=False is required for Streamlit + Background scripts
engine = create_engine(sqlite_url, connect_args={"check_same_thread": False})

class Bid(SQLModel, table=True):
    id: int | None = Field(default=None, primary_key=True)
    title: str
    url: str = Field(unique=True, index=True)
    source: str
    status: str = Field(default="pending")
    # Statuses: downloaded -> filtered -> qualified -> drafted -> rejected

    local_path: str | None = None
    relevance_score: int = 0
    relevance_reason: str | None = None
    draft_content: str | None = None
    created_at: datetime = Field(default_factory=datetime.now)

def init_db():
    # Enable Write-Ahead Logging (WAL) for concurrency
    with engine.connect() as con:
        con.exec_driver_sql("PRAGMA journal_mode=WAL;")
    SQLModel.metadata.create_all(engine)

if __name__ == "__main__":
    init_db()
    print("‚úÖ Database initialized.")

```

**Action:** Run `python db_manager.py` once to create the DB.

---

### **Phase 3: The Harvester (Ingestion)**

**File:** `services/harvester.py`
Downloads PDFs. It filters by keyword _first_ to avoid wasting Gemini quota on "Asphalt Paving" bids.

```python
import time
import feedparser
import requests
import os
from pathlib import Path
from db_manager import Bid, engine, Session

# Setup
DOWNLOAD_DIR = Path("C:/GovBids/data")
DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)

RSS_FEEDS = [
    ("CanadaBuys", "https://canadabuys.canada.ca/en/tender-notices/feed"),
    # Add more feeds here
]

def sanitize(title):
    return "".join([c for c in title if c.isalpha() or c.isdigit() or c==' ']).strip()[:50]

def run_harvester():
    print("üåæ Harvester: Scanning feeds...")
    with Session(engine) as session:
        for source, feed_url in RSS_FEEDS:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries:
                    # 1. Deduplication
                    if session.query(Bid).filter(Bid.url == entry.link).first():
                        continue

                    # 2. keyword Filter (Save Bandwidth)
                    keywords = ['software', 'data', 'cloud', 'platform', 'app', 'system']
                    if not any(k in entry.title.lower() for k in keywords):
                        continue

                    print(f"üì• Found: {entry.title}")

                    # 3. Download (Mock logic for feed links)
                    # In production, you might need Playwright if the link is a wrapper page
                    fname = f"{sanitize(entry.title)}.pdf"
                    fpath = DOWNLOAD_DIR / fname

                    # For this demo, we assume the link IS the pdf or we save the HTML description
                    # Real implementation: requests.get(entry.link)
                    fpath.write_text(f"Title: {entry.title}\nDesc: {entry.summary}")

                    new_bid = Bid(title=entry.title, url=entry.link, source=source,
                                  status="downloaded", local_path=str(fpath))
                    session.add(new_bid)
                    session.commit()

            except Exception as e:
                print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    while True:
        run_harvester()
        time.sleep(3600) # Run hourly

```

---

### **Phase 4: The Gemini Processor (The Brain)**

**File:** `services/processor.py`
This is the core. It uses Gemini to "Read" (Filter) and "Write" (Draft).
It includes a **Rate Limiter** to respect the Free Tier (15 requests/min).

```python
import time
import os
import google.generativeai as genai
from dotenv import load_dotenv
from db_manager import Bid, engine, Session, select

load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

def upload_to_gemini(path, mime_type="application/pdf"):
    """Uploads file to Gemini (Temporary storage)"""
    file = genai.upload_file(path, mime_type=mime_type)
    # Wait for processing
    while file.state.name == "PROCESSING":
        time.sleep(2)
        file = genai.get_file(file.name)
    return file

def analyze_bid(bid_id):
    with Session(engine) as session:
        bid = session.get(Bid, bid_id)
        if not bid: return

        print(f"üß† Analyzing: {bid.title}")

        # 1. Upload File (If PDF, use 'application/pdf', else 'text/plain')
        # We assume text for this mock, but Gemini handles PDFs natively!
        gemini_file = upload_to_gemini(bid.local_path, mime_type="text/plain")

        # 2. FILTERING (Use Flash - Fast/Cheap)
        model = genai.GenerativeModel("gemini-1.5-flash")

        prompt = """
        Analyze this government tender.
        1. Is this primarily for CUSTOM SOFTWARE development? (Yes/No)
        2. Give a relevance score (0-100).
        3. Explain why.
        Format: JSON { "is_software": bool, "score": int, "reason": "str" }
        """

        result = model.generate_content([gemini_file, prompt])
        # (Add JSON parsing logic here in prod)
        print(f"   ü§ñ Analysis: {result.text[:100]}...")

        # Mock parsing for simplicity
        is_software = "true" in result.text.lower()

        if is_software:
            bid.status = "qualified"
            bid.relevance_score = 85
        else:
            bid.status = "rejected"
            bid.relevance_score = 10

        session.add(bid)
        session.commit()

        # Rate Limit Sleep (Free tier = 15 RPM = 4s delay)
        time.sleep(4)

def run_processor():
    print("üè≠ Processor started...")
    while True:
        with Session(engine) as session:
            # Find next downloaded bid
            bid = session.exec(select(Bid).where(Bid.status == "downloaded")).first()

        if bid:
            analyze_bid(bid.id)
        else:
            time.sleep(5) # Idle wait

if __name__ == "__main__":
    run_processor()

```

---

### **Phase 5: The Dashboard (Streamlit)**

**File:** `app.py`
Run this to see your pipeline in action. It triggers the **Drafting** step on demand.

```python
import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv
from db_manager import Bid, engine, Session, select

load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

st.set_page_config(layout="wide", page_title="Gemini GovBid")
st.title("üèõÔ∏è Gemini GovBid Pipeline")

def generate_draft(bid):
    """Uses Gemini Pro for high-quality writing"""
    model = genai.GenerativeModel("gemini-1.5-pro")

    # Re-upload or use cached content
    # In prod, cache the file URI in DB to avoid re-uploading
    gemini_file = genai.upload_file(bid.local_path, mime_type="text/plain")

    prompt = "You are a proposal writer. Write a Technical Approach based on this RFP."
    response = model.generate_content([gemini_file, prompt])
    return response.text

with Session(engine) as session:
    # Stats
    col1, col2, col3 = st.columns(3)
    col1.metric("Pending", session.query(Bid).filter(Bid.status == "downloaded").count())
    col2.metric("Qualified", session.query(Bid).filter(Bid.status == "qualified").count())
    col3.metric("Drafted", session.query(Bid).filter(Bid.status == "drafted").count())

    st.divider()

    # Qualified Feed
    st.subheader("üéØ Qualified Opportunities")
    bids = session.exec(select(Bid).where(Bid.status == "qualified")).all()

    for bid in bids:
        with st.expander(f"{bid.title} (Score: {bid.relevance_score})"):
            st.write(bid.url)
            if st.button("Generate Proposal ‚ú®", key=bid.id):
                with st.spinner("Gemini Pro is writing..."):
                    draft = generate_draft(bid)
                    bid.draft_content = draft
                    bid.status = "drafted"
                    session.add(bid)
                    session.commit()
                    st.success("Draft created!")
                    st.rerun()

    # Drafts
    st.subheader("üìù Ready Drafts")
    drafts = session.exec(select(Bid).where(Bid.status == "drafted")).all()
    for d in drafts:
        with st.expander(f"Draft: {d.title}"):
            st.markdown(d.draft_content)

```

---

### **Phase 6: The Orchestrator**

**File:** `main.py`
This script starts everything.

```python
import subprocess
import sys
import time

def main():
    print("üöÄ Launching Gemini GovBid System...")

    processes = []

    # 1. Harvester (Background)
    processes.append(subprocess.Popen([sys.executable, "services/harvester.py"]))

    # 2. Processor (Background)
    processes.append(subprocess.Popen([sys.executable, "services/processor.py"]))

    # 3. Streamlit UI (Foreground)
    processes.append(subprocess.Popen(["streamlit", "run", "app.py"]))

    print("‚úÖ System Running. Press Ctrl+C to stop.")

    try:
        while True: time.sleep(1)
    except KeyboardInterrupt:
        print("üõë Shutting down...")
        for p in processes: p.terminate()

if __name__ == "__main__":
    main()

```

---

### **Execution Checklist**

1. **API Key:** Ensure `.env` has your key.
2. **Database:** Run `python db_manager.py` to create the file.
3. **Run:** Execute `python main.py`.

This setup gives you a **free, enterprise-grade AI pipeline** running entirely on your local machine, powered by Google's massive infrastructure.
